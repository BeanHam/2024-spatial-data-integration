{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d37fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c5a9d2-e256-416e-b99c-a568dc10ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calculation(pred, gt):    \n",
    "    acc=accuracy_score(gt, pred)\n",
    "    f1=f1_score(gt, pred, average='macro')\n",
    "    confusion=confusion_matrix(gt, pred)\n",
    "    fpr=confusion[0,1]/len(gt) ## predict to be 1; actual 0\n",
    "    fnr=confusion[1,0]/len(gt) ## predict to be 0; actual 1\n",
    "    return acc, f1, fpr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afb2d5c-7e55-4e7a-bfcb-29c98667467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(pred):\n",
    "    new_pred=[]\n",
    "    for i in pred:\n",
    "        i=i.lower()\n",
    "        if 'response' in i:\n",
    "            try: new_pred.append(i.split('response')[1].split()[1].replace('</s>', ''))\n",
    "            except: new_pred.append(2)\n",
    "        elif 'output' in i:\n",
    "            try: new_pred.append(i.split('output')[1].split()[1].replace('</s>', ''))\n",
    "            except: new_pred.append(2)\n",
    "        else:\n",
    "            try: new_pred.append(i.split()[0].replace('</s>', ''))\n",
    "            except:new_pred.append(2)\n",
    "    new_pred = np.array([int(float(i)) if i in ['0', '0.0', '1', '1.0'] else 2 for i in new_pred])\n",
    "    return new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad1aa0e-fe18-458e-b5b5-70df9e1443fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"beanham/spatial_union_dataset\")\n",
    "test=ds['test']\n",
    "gt=np.array(test['label'])\n",
    "configs = [\n",
    "    'zero_shot_no_heur',    \n",
    "    'zero_shot_with_heur_hint_angle',\n",
    "    'zero_shot_with_heur_hint_area',\n",
    "    'zero_shot_with_heur_hint_angle_area',\n",
    "    'zero_shot_with_heur_value_angle',\n",
    "    'zero_shot_with_heur_value_area',\n",
    "    'zero_shot_with_heur_value_angle_area',\n",
    "    'few_shot_no_heur',    \n",
    "    'few_shot_with_heur_hint_angle',\n",
    "    'few_shot_with_heur_hint_area',\n",
    "    'few_shot_with_heur_hint_angle_area',\n",
    "    'few_shot_with_heur_value_angle',\n",
    "    'few_shot_with_heur_value_area',\n",
    "    'few_shot_with_heur_value_angle_area'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9c7def-149f-4ea9-95c7-47aa29b4dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3...\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "models=['llama3']\n",
    "for model in models:\n",
    "    print(f'Model: {model}...')\n",
    "    for config in configs:\n",
    "        pred=np.load(f'base/{model}/{model}_{config}.npy')\n",
    "        pred=post_processing(pred)        \n",
    "        metrics=metric_calculation(pred, gt)\n",
    "        results.append([config, model, round(metrics[0],3), metrics[1]])\n",
    "results=pd.DataFrame(results, columns=['config', 'model', 'acc', 'f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
